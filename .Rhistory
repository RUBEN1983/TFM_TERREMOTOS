mi.lista <- list(a=1:10, b=letters[1:3], cc=c(TRUE,FALSE,TRUE,FALSE))
mi.lista
length(mi.lista)
tapply(mi.lista,length)
lapply(mi.lista,length)
mi.lista[1]
sapply(mi.lista,length)
data[0]
data[1]
garbanzo <- lapply(data,extrae.hora)
length(mi.lista[1])
length(mi.lista[2])
mi.lista[1]
length(mi.lista[3])
length(mi.lista)
length(mi.lista[1][1])
sapply(mi.lista,length)
mi.lista[1][6]
mi.lista[1]
mi.lista[[1]]
mi.lista[[1]][6]
garbanzo <- lapply(data,class)
garbanzo
length(garbanzo)
data <- xml_data$eventParameters$event
garbanzo <- lapply(data,class)
length(garbanzo)
garbanzo
extrae.hora <- function (x){
x$origin$time$value
}
extrae.hora(data[100])
extrae.hora(data)
extrae.hora <- function (x){
print x$origin$time$value
}
extrae.hora <- function (x){
print (x$origin$time$value)
}
garbanzo <- lapply(data,class)
garbanzo
garbanzo <- lapply(data,extrae.hora)
data[1]
data[2]
data[3]
len(data)
length(data)
data
data[5]
sapply(mi.lista,length)
garbanzo <- lapply(data,extrae.hora)
extrae.hora <- function (x){
hora <- x$event$origin$time$value
hora
}
garbanzo <- lapply(data,extrae.hora)
xml_data$eventParameters[10]$event$origin$time$value
xml_data$eventParameters[123]$event$origin$time$value
extrae.hora <- function (x){
hora <- x[["event"]][["origin"]][["time"]][["value"]]
hora
}
garbanzo <- lapply(data,extrae.hora)
extrae.hora(data[2])
print hora
extrae.hora <- function (x){
hora <- x[["event"]][["origin"]][["time"]][["value"]]
print (hora)
}
extrae.hora(data)
extrae.hora(data[1])
extrae.hora(data[2])
extrae.hora(data[34])
extrae.hora <- function (x){
hora <- x[["event"]]
print (hora)
}
extrae.hora(data[34])
extrae.hora <- function (x){
hora <- x$event
print (hora)
}
extrae.hora(data[34])
extrae.hora <- function (x){
x$event
}
extrae.hora(data[34])
data
data[34]
data <- xml_data$eventParameters
data[34]
extrae.hora <- function (x){
x[["event"]]
}
extrae.hora(data[34])
extrae.hora <- function (x){
x[["event"]][["origin"]]
}
extrae.hora(data[34])
x[["event"]][["origin"]][["time"]]
extrae.hora <- function (x){
x[["event"]][["origin"]][["time"]]
}
extrae.hora(data[34])
extrae.hora <- function (x){
x[["event"]][["origin"]][["time"]][["value"]]
}
extrae.hora(data[34])
garbanzo <- lapply(data,extrae.hora)
extrae.hora(data[34])
print (data[x]$event$origin$time$value)
for x in data{print (data[x]$event$origin$time$value)}
for x in data: {print (data[x]$event$origin$time$value)}
for (x in data): {print (data[x]$event$origin$time$value)}
for (x in data) {print (data[x]$event$origin$time$value)}
for (x in 1:x) {print (data[x]$event$origin$time$value)}
length(data)
for (x in 1:length(data)) {print (data[x]$event$origin$time$value)}
class(xml)
class(xml_data)
length(xml_data)
length(xml_data$eventParameters)
garbanzo <- lapply(xml_data$eventParameters,extrae.hora)
x[["event"]]
extrae.hora <- function (x){
x[["event"]]
}
garbanzo <- lapply(xml_data$eventParameters,extrae.hora)
library(XML)
library(zoo)
install.packages("zoo")
library(zoo)
mainURL <- 'http://www.tiempodiario.com'
radURL <- 'http://www.tiempodiario.com/radiacion/'
days <- seq(as.POSIXct('2010-12-05'), as.POSIXct('2012-10-28'), by='day')
for (i in seq_along(days)){
day <- days[i]
dayURL <- paste(radURL, format(day, '%Y/%m/%d'), '/', sep='')
## Obtengo el contenido HTML de la página del día en cuestión
content <- htmlParse(dayURL, encoding='utf8')
## y extraigo los enlaces a cada estación para ese día
links <- xpathSApply(content, "//a/@href")
linksData <- links[grep('radiacion-estacion', links)]
## Los nombres de las estaciones están escritos en los encabezados H2
h2 <- xpathSApply(content, "//h2")
namesStations <- sapply(h2[-1], function(s){
val <- xmlValue(s)
substr(val, 10, nchar(val))
})
## pero sus códigos de identificación están en los enlaces correspondientes
idStation <- strsplit(linksData, '/')
idStations <- sapply(idStation, function(x)x[4])
dayStationURL <- paste(mainURL, linksData, sep='')
## Accedo a la página de cada estación para ese día
zList <- lapply(dayStationURL, function(ds){
print(ds)
## Obtengo el contenido HTML de la página
content <- htmlParse(ds, encoding='utf8')
## y extraigo las tablas
tabs <- readHTMLTable(content, header=TRUE)
## Cada tabla codifica el índice temporal en la primera
## columna. A pesar de usar header=TRUE, la primera fila
## contiene el nombre las columnas. De ahí que tenga que usar
## -1 en el indexado.
time <- tabs[[1]][-1,1]
dayTime <- format(as.POSIXct(paste(day, time)), '%Y-%m-%d %H:%M')
## El nombre de la variable de cada tabla está codificado en
## los encabezados H2
h2 <- getNodeSet(content, '//h2')
tabLabels <- sapply(h2, xmlValue)
## Sólo me quedaré con las radiación difusa, directa y global
radVars <- c('Radiación Difusa (DF)',
'Radiación Directa (DT)',
'Radiación Global (GL)')
whichTabs <- which(tabLabels %in% radVars)
## Extraigo la información de las tablas seleccionadas. Dado
## que la primera fila contiene los nombres de las columnas,
## todo el contenido es leído como un "factor". De ahí que
## tenga que usar la conversión as.character y después
## as.numeric.
dat <- sapply(tabs[whichTabs],
function(tab)as.numeric(as.character(tab[-1, 2])))
## Finalmente, si todo ha ido bien, construyo un objeto zoo
## para almacenar la serie temporal de esta estación para este
## día. Si algo no ha funcionado obtendré NULL.
if (length(dat) > 0 & !is.list(dat)) {
z <- zoo(dat, as.POSIXct(dayTime))
names(z) <- tabLabels[whichTabs]
} else {
z <- NULL
}
z
})
## El resultado de lapply será una lista de objetos zoo, uno
## para cada estación. Pongo los nombres de las estaciones a
## cada componente de esta lista y grabo el resultado como un
## fichero RData.
names(zList) <- make.names(namesStations)
save(zList, file=paste(day, 'RData', sep='.'))
## Este código se repetirá para cada día, de forma que obtendré
## un conjunto de ficheros RData, uno por día. Cada uno de ellos
## contiene una lista de zoo's, correspondiendo cada componente
## de la lista a una estación de medida.
}
url.ibex <- "http://seismicportal.eu/"
tmp <- read_html(url.ibex)
??read_html
install.packages("xml2")
library(xml2)
url.ibex <- "http://seismicportal.eu/"
tmp <- read_html(url.ibex)
tmp <- html_nodes(tmp, "table")
library(rvest)
tmp <- html_nodes(tmp, "table")
tmp
require(xml2)
data <- xmlParse("http://www.seismicportal.eu/fdsnws/event/1/query?start=2011-01-01&end=2011-01-02")
data
class(data)
xml_data <- xmlToDataFrame(data)
xml_data
dim(xml_data)
xml_data <- xmlToS4(data)
data <- xmlParse("http://www.seismicportal.eu/fdsnws/event/1/query?start=2011-01-01&end=2011-01-02")
xml_data <- xmlToS4(data)
xml_data <- xmlTextNode(data)
xml_data <- xmlToList(data)
xml_data$eventParameters[1]
xml_data$eventParameters[2]
readLines(xml_data[1])
readLines(xml_data[[1]])
dataframe <- xmlToDataFrame(xml_data$eventParameters$event)
xml_data$eventParameters[2]
xml_data$eventParameters[2]$event
class(xml_data$eventParameters[2]$event)
install.packages("dummies")
library(dummies)
library(ggplot2)
install.packages("ggplot2")
library(dummies)
library(ggplot2)
coches=mtcars # Base de datos ejemplo en R
coches
str(coches)
head(coches)
summary(coches)
modelo_bruto=lm(mpg~.,data=coches) ##Regresion lineal
summary(modelo_bruto)
write.table(coches,"coches.csv", sep = ";")
cor(coches)
step(modelo_bruto,direction = "backward",trace=1)
modelo1=lm(mpg~cyl,data=coches)
summary(modelo1)
modelo2=lm(mpg~disp,data=coches)
summary(modelo2)
modelo3=lm(mpg~hp,data=coches)
summary(modelo3)
modelo4=lm(mpg~drat,data=coches)
summary(modelo4)
modelo5=lm(mpg~wt,data=coches)
summary(modelo5)
modelo6=lm(mpg~qsec,data=coches)
summary(modelo6)
modelo7=lm(mpg~vs,data=coches)
summary(modelo7)
modelo8=lm(mpg~am,data=coches)
summary(modelo8)
modelo9=lm(mpg~gear,data=coches)
summary(modelo9)
modelo10=lm(mpg~carb,data=coches)
summary(modelo10)
cor(coches)
modelo11=lm(mpg~  wt+qsec+am,data=coches) ##las que tengan mas correlacion con el target, en este caso mpg, usamos step backward para sacar los valores
summary(modelo11)
PCA<-prcomp(coches[,-c(1)],scale. = TRUE) ##Para sacar cuanto aporta cada variable, es como el codo
PCA
summary(PCA)
plot(PCA)
cor(coches)
cor(PCA$x)
biplot(PCA)
PCA$rotation
coches$PCA1=PCA$x[,1]
coches$PCA2=PCA$x[,2]
coches$PCA3=PCA$x[,3]
coches
modelo_PCA=lm(mpg~PCA1,data=coches)
summary(modelo_PCA)
modelo_PCA=lm(mpg~PCA$x,data=coches)
summary(modelo_PCA)
modelo_PCA=lm(mpg~PCA1+PCA3,data=coches)
summary(modelo_PCA)
biplot(PCA,choices=c(1,3))
coches=mtcars # Base de datos ejemplo en R
coches
str(coches)
head(coches)
summary(coches)
modelo_bruto=lm(mpg~.,data=coches) ##Regresion lineal
summary(modelo_bruto)
write.table(coches,"coches.csv", sep = ";")
cor(coches)
step(modelo_bruto,direction = "backward",trace=1)
modelo1=lm(mpg~cyl,data=coches)
summary(modelo1)
head(coches)
modelo2=lm(mpg~disp,data=coches)
summary(modelo2)
modelo3=lm(mpg~hp,data=coches)
summary(modelo3)
modelo4=lm(mpg~drat,data=coches)
summary(modelo4)
modelo5=lm(mpg~wt,data=coches)
summary(modelo5)
modelo6=lm(mpg~qsec,data=coches)
summary(modelo6)
modelo7=lm(mpg~vs,data=coches)
summary(modelo7)
modelo8=lm(mpg~am,data=coches)
summary(modelo8)
modelo9=lm(mpg~gear,data=coches)
summary(modelo9)
modelo10=lm(mpg~carb,data=coches)
summary(modelo10)
cor(coches)
modelo11=lm(mpg~  wt+cyl+disp,data=coches) ##las que tengan mas correlacion con el target, en este caso mpg, usamos step backward para sacar los valores
summary(modelo11)
modelo11=lm(mpg~  wt+qsec+am,data=coches) ##las que tengan mas correlacion con el target, en este caso mpg, usamos step backward para sacar los valores
summary(modelo11)
PCA<-prcomp(coches[,-c(1)],scale. = TRUE) ##Para sacar cuanto aporta cada variable, es como el codo
PCA
summary(PCA)
plot(PCA)
cor(coches)
cor(PCA$x)
biplot(PCA)
PCA$rotation
PCA$x
coches$PCA1=PCA$x[,1]
coches$PCA2=PCA$x[,2]
coches$PCA3=PCA$x[,3]
coches
modelo_PCA=lm(mpg~PCA1,data=coches)
summary(modelo_PCA)
modelo_PCA=lm(mpg~PCA$x,data=coches)
summary(modelo_PCA)
modelo_PCA=lm(mpg~PCA1+PCA3,data=coches)
summary(modelo_PCA)
biplot(PCA,choices=c(1,3))
library(ggplot2)
library(effects)
library(plyr)
load("data/samsungData.rda")
install.packages("effects")
library(ggplot2)
library(effects)
library(plyr)
load("data/samsungData.rda")
str(samsungData)
samsungData <- load("data/samsungData.rda")
install.packages("ggmap")
library(ggmap)
library(ggplot2)
unizar <- geocode('Plaza de la Escandalera, Oviedo', source = "google")
unizar         # coordenadas de Plaza de la Escandalera, Oviedo
unizar <- geocode('40.0,40.0', source = "google")
install.packages("dplyr")
library(ROCR)
library(reshape2)
library(randomForest)
setwd("C:/Users/Ruben/Desktop/Master KSCHOOL/TFM_TERREMOTOS/")
dir()
terremotos_paises <- read.csv('files/paises_prediccion_valores.csv',sep = ';')
terremotos_paises$X <- NULL
head(terremotos_paises)
terremotos_paises_ordenado <- dcast(terremotos_paises,paises ~ year)
head(terremotos_paises_ordenado)
colnames(terremotos_paises_ordenado) <- c("coordenadas","X1998","X1999","X2000","X2001","X2002","X2003","X2004","X2005","X2006","X2007","X2008","X2009","X2010","X2011","X2012","X2013","X2014","X2015","X2016")
terremotos_paises_ordenado$media <- round(apply(terremotos_paises_ordenado[,2:20],1,mean))
rmspe <- function(predictions, target) {
x1 <- target - predictions
x2 <- x1 / target
x2[target==0] <- 0
x3 <- x2*x2
x4 <- sum(x3)
x5 <- (x4/length(target))
x6 <- sqrt(x5)
x6
}
regresionLOG <- glm(media~X2016+X2015+X2014+X2013+X2012+X2011+X2010+X2009+X2008+X2007+X2006+X2005+X2004+X2003+X2002+X2001+X2000+X1999+X1998, data = terremotos_paises_ordenado, family = "poisson")
summary(regresionLOG)
terremotos_paises_ordenado$predictLMLOG=round(exp(predict(regresionLOG, terremotos_paises_ordenado)))
terremotos_paises_ordenado$predictLMLOG <- as.numeric(terremotos_paises_ordenado$predictLMLOG)
rmspe(terremotos_paises_ordenado$predictLMLOG,terremotos_paises_ordenado$media)
set.seed(1234)
modelRF=randomForest(terremotos_paises_ordenado[,c(2:20)],
terremotos_paises_ordenado$media,
ntree=200,
sampsize=150,
do.trace=TRUE)
terremotos_paises_ordenado$predictRF=round(predict(modelRF, terremotos_paises_ordenado))
terremotos_paises_ordenado$predictRF <- as.numeric(terremotos_paises_ordenado$predictRF)
rmspe(terremotos_paises_ordenado$predictRF,terremotos_paises_ordenado$media)
set.seed(1234)
modelRFlog=randomForest(terremotos_paises_ordenado[,c(2:20)],
log(terremotos_paises_ordenado$media+1),
ntree=1000,
sampsize=150,
do.trace=TRUE)
terremotos_paises_ordenado$predictRFlog=round(exp(predict(modelRFlog, terremotos_paises_ordenado)))
terremotos_paises_ordenado$predictRFlog <- as.numeric(terremotos_paises_ordenado$predictRFlog)
rmspe(terremotos_paises_ordenado$predictRFlog,terremotos_paises_ordenado$media)
modelRF=randomForest(terremotos_paises_ordenado[,c(2:20)],
terremotos_paises_ordenado$media,
ntree=1000,
sampsize=150,
do.trace=TRUE)
terremotos_paises_ordenado$predictRF=round(predict(modelRF, terremotos_paises_ordenado))
terremotos_paises_ordenado$predictRF <- as.numeric(terremotos_paises_ordenado$predictRF)
rmspe(terremotos_paises_ordenado$predictRF,terremotos_paises_ordenado$media)
set.seed(1234)
modelRFlog=randomForest(terremotos_paises_ordenado[,c(2:20)],
log(terremotos_paises_ordenado$media+1),
ntree=200,
sampsize=150,
do.trace=TRUE)
terremotos_paises_ordenado$predictRFlog=round(exp(predict(modelRFlog, terremotos_paises_ordenado)))
terremotos_paises_ordenado$predictRFlog <- as.numeric(terremotos_paises_ordenado$predictRFlog)
rmspe(terremotos_paises_ordenado$predictRFlog,terremotos_paises_ordenado$media)
set.seed(1234)
modelRFlog=randomForest(terremotos_paises_ordenado[,c(2:20)],
log(terremotos_paises_ordenado$media+1),
ntree=100,
sampsize=150,
do.trace=TRUE)
terremotos_paises_ordenado$predictRFlog=round(exp(predict(modelRFlog, terremotos_paises_ordenado)))
terremotos_paises_ordenado$predictRFlog <- as.numeric(terremotos_paises_ordenado$predictRFlog)
rmspe(terremotos_paises_ordenado$predictRFlog,terremotos_paises_ordenado$media)
write.csv(terremotos_paises_ordenado,"files/prediccion_terremotos_coordenadas.csv")
library(reshape2)
library(randomForest)
##Modificamos a la ruta de nuestro proyecto
setwd("C:/Users/Ruben/Desktop/Master KSCHOOL/TFM_TERREMOTOS/")
dir()
###Cargamos los datos con los terremotos de 1998 a 2016
terremotos_coordenadas <- read.csv('files/coordenadas_prediccion_valores.csv',sep = ';')
##Eliminamos la columna X que se crea al cargar el fichero
terremotos_coordenadas$X <- NULL
head(terremotos_coordenadas)
###Ordenamos los datos con la función dcast para obtener una tabla con los terremotos por año
terremotos_coordenadas_ordenado <- dcast(terremotos_coordenadas,coordenadas ~ year)
head(terremotos_coordenadas_ordenado)
##Cambiamos el nombre a las columnas. Las columnas con nombre de número dan problemas
colnames(terremotos_coordenadas_ordenado) <- c("coordenadas","X1998","X1999","X2000","X2001","X2002","X2003","X2004","X2005","X2006","X2007","X2008","X2009","X2010","X2011","X2012","X2013","X2014","X2015","X2016")
##Incluimos una columna con la media de terremotos anual. Este sera el valor al que intentaremos predecir.
terremotos_coordenadas_ordenado$media <- round(apply(terremotos_coordenadas_ordenado[,2:20],1,mean))
#####ALGORITMOS PREDICTIVOS
###Función rmspe para calcular la exactitud de la predicción; cuanto más cercano a 0, más fiel será nuestra predicción
rmspe <- function(predictions, target) {
x1 <- target - predictions
x2 <- x1 / target
x2[target==0] <- 0
x3 <- x2*x2
x4 <- sum(x3)
x5 <- (x4/length(target))
x6 <- sqrt(x5)
x6
}
#########Modelo con regresion logistica y resumen del resultado
regresionLOG <- glm(media~X2016+X2015+X2014+X2013+X2012+X2011+X2010+X2009+X2008+X2007+X2006+X2005+X2004+X2003+X2002+X2001+X2000+X1999+X1998, data = terremotos_coordenadas_ordenado, family = "poisson")
summary(regresionLOG)
##Aplicación del modelo de predicción y cálculo de rmspe
terremotos_coordenadas_ordenado$predictLMLOG=round(exp(predict(regresionLOG, terremotos_coordenadas_ordenado)))
terremotos_coordenadas_ordenado$predictLMLOG <- as.numeric(terremotos_coordenadas_ordenado$predictLMLOG)
rmspe(terremotos_coordenadas_ordenado$predictLMLOG,terremotos_coordenadas_ordenado$media)
####Modelo RandomForest lineal
set.seed(1234)
##Modelo RandomForest con 50 árboles de decisión y 8000 muestras
modelRF=randomForest(terremotos_coordenadas_ordenado[,c(2:20)],
terremotos_coordenadas_ordenado$media,
ntree=50,
sampsize=8000,
do.trace=TRUE)
##Aplicación del modelo de predicción y cálculo de rmspe
terremotos_coordenadas_ordenado$predictRF=round(predict(modelRF, terremotos_coordenadas_ordenado))
terremotos_coordenadas_ordenado$predictRF <- as.numeric(terremotos_coordenadas_ordenado$predictRF)
rmspe(terremotos_coordenadas_ordenado$predictRF,terremotos_coordenadas_ordenado$media)
###Modelo RandomForest logarítmico
set.seed(1234)
##Modelo RandomForest con 50 árboles de decisión y 8000 muestras
modelRFlog=randomForest(terremotos_coordenadas_ordenado[,c(2:20)],
log(terremotos_coordenadas_ordenado$media+1),
ntree=50,
sampsize=8000,
do.trace=TRUE)
##Aplicación del modelo de predicción y cálculo de rmspe
terremotos_coordenadas_ordenado$predictRFlog=round(exp(predict(modelRFlog, terremotos_coordenadas_ordenado)))
terremotos_coordenadas_ordenado$predictRFlog <- as.numeric(terremotos_coordenadas_ordenado$predictRFlog)
rmspe(terremotos_coordenadas_ordenado$predictRFlog,terremotos_coordenadas_ordenado$media)
##Guardamos el fichero con la predicción. Posteriormente se usará para tableau.
write.csv(terremotos_coordenadas_ordenado,"files/prediccion_terremotos_coordenadas.csv")
set.seed(1234)
modelRFlog=randomForest(terremotos_paises_ordenado[,c(2:20)],
log(terremotos_paises_ordenado$media+1),
ntree=200,
sampsize=150,
do.trace=TRUE)
terremotos_paises_ordenado$predictRFlog=round(exp(predict(modelRFlog, terremotos_paises_ordenado)))
terremotos_paises_ordenado$predictRFlog <- as.numeric(terremotos_paises_ordenado$predictRFlog)
rmspe(terremotos_paises_ordenado$predictRFlog,terremotos_paises_ordenado$media)
write.csv(terremotos_paises_ordenado,"files/prediccion_terremotos_paises.csv")
View(terremotos_paises_ordenado)
